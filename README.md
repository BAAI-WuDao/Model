# Model
“悟道”项目现有7个开源模型成果，本仓库提供所有开源成果的模型参数文件链接。

* **[GLM](https://wudaoai.cn/model/detail/GLM系列)**

  GLM (General Language Model) 是一个全新的预训练框架，打破BERT和GPT的瓶颈。单一GLM模型在语言理解和生成任务方面取得了最佳结果，并且超过了对相同数据量进行训练的常见预训练模型（例如BERT，RoBERTa和T5）。

* **[Transformer-XL](https://wudaoai.cn/model/detail/Transformer-XL)**

  Transformers具有长期学习依赖的潜力，但在语言建模设置中受到固定长度上下文的限制。而Transformer-XL作为一种新神经网络架构可以很好地解决长文本依赖问题。悟道基于Transformer-XL训练并开放29亿的语言模型，在长文本生成方面具有优势。
 
* **[CPM](https://wudaoai.cn/model/detail/CPM系列)**

  2020年11月，CPM系列正式发布，并推出当时参数最大的26亿中文预训练语言模型CPM-1（Chinese Pretrained Models），可支持简单对话、文章生成和语言理解等下游任务。2021年6月，CPM-2（Cost-efficient Pre-trained language Models）在北京智源大会上发布，推出了110亿的中英双语语言模型和对应的1980亿MoE版模型。
  
* **[CogView](https://wudaoai.cn/model/detail/CogView)**

  世界最大的中文多模态生成模型，参数量为40亿。模型支持文生成图为基础的多领域下游任务，在应用维度上具备通用性，经过微调后可实现国画、油画、水彩画、轮廓画等图像生成。

* **[BriVL](https://wudaoai.cn/model/detail/BriVL)**

  BriVL (Bridging Vision and Language Model) 是首个中文通用图文多模态大规模预训练模型。BriVL模型在图文检索任务上有着优异的效果，超过了同期其他常见的多模态预训练模型（例如UNITER、CLIP）。
  
* **[ProteinLM](https://wudaoai.cn/model/detail/ProteinLM)**

  蛋白质序列预训练模型，目前已开源2亿和30亿参数规模的模型，能支持蛋白质二级结构预测、荧光性预测、接触预测、折叠稳定性预测和远缘同源性检测任务。相较于基线模型TAPE（3800万参数），我们的模型在下游任务上表现有所提升，尤其是在蛋白质折叠问题的接触预测问题上，模型较基线模型提高了16%。

* **[Lawformer](https://wudaoai.cn/model/detail/Lawformer)**

  在数千万的法律文书上训练能够处理法律长文本的预训练语言模型，参数规模为1亿，支持中文法律长文本输入的理解任务。
  
